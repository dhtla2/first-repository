{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "earlier-shock",
   "metadata": {},
   "source": [
    "# 1. 필요한 라이브러리 import 및 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handled-austin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\", \"I can't get no relief Business men, they drink my wine\", 'Plowman dig my earth', 'None were level on the mind', 'Nobody up at his word', 'Hey, hey No reason to get excited', 'The thief he kindly spoke', 'There are many here among us']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-farming",
   "metadata": {},
   "source": [
    "# 2. 문장 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documented-above",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    if len(sentence) == 0 :\n",
    "        return 0\n",
    "    elif sentence.count(\" \") > 12:\n",
    "        return 0\n",
    "    else:\n",
    "        sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "        return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-breakfast",
   "metadata": {},
   "source": [
    "## 함수를 만들때, 팀원의 도움을 받았습니다. \n",
    "### 처음에 제가 만들때는 함수를 만들고 뒤에가서 걸러 내려고 했는데, 함수를 짜면서 원하지 않는 문장을 바로 0으로 리턴해 그 다음에 필요한 문장만 뽑는다면 메모리와 처리 속도를 올릴 수 있어 필요한 문장을 뽑는 함수를 만들었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-gossip",
   "metadata": {},
   "source": [
    "# 3. 정제된 문장을 모으고 일부 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "improved-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if preprocessed_sentence == 0:\n",
    "        pass\n",
    "    else:\n",
    "        corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-vietnamese",
   "metadata": {},
   "source": [
    "# 4. 토크나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flexible-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 271 ...   0   0   0]\n",
      " [  2 117   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  75  45 ...   3   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 635 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fdf0a7e8990>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-witch",
   "metadata": {},
   "source": [
    "## 여러 옵션들을 텐서플로우 튜토리얼에서 확인했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-control",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/84653776/135948844-6df468fc-a50c-427a-88da-20466a1f0b16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-edmonton",
   "metadata": {},
   "source": [
    "## 문장이 잘 바뀌었나 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "educational-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   62  271   27   94  546   20   86  743   90]\n",
      " [   2  117    6 6269   10    6 2310    3    0    0]\n",
      " [   2   62   17  102  184 2718    3    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-mainstream",
   "metadata": {},
   "source": [
    "## 생성된 단어장을 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vietnamese-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-customer",
   "metadata": {},
   "source": [
    "### 단어장을 생성해 보았는데 여러번 돌려도 같은 순서같고, 12000단어의 중요도를 어떻게 판단하는지 궁금했는데 수요일 TF 마스터 수업에서 조원의 도움으로 알게되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "flexible-invasion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = tokenizer.get_config()\n",
    "### print(config)  너무 길어서 짤라서 올리겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-stephen",
   "metadata": {},
   "source": [
    "{'num_words': 12000, 'filters': ' ', 'lower': True, 'split': ' ', 'char_level': False, 'oov_token': '<unk>', 'document_count': 156174, 'word_counts': '{\"<'star't>\": 156174, \"there\": 3040, \"must\": 509, \"be\": 6646, \"some\": 1847, \"kind\": 224, \"of\": 8964, \"way\": 2081, \"outta\": 145, \"here\": 1914, \"<'end'>\": 156174, \"said\": 1485, \"the\": 39314, \"joker\": 7, \"to\": 20368, \"thief\": 31, \"s\": 11126, \"too\": 1681, \"much\": 818, \"confusion\": 25, \"i\": 45489, \"can\": 5616, \"t\": 11207, \"get\": 4928, \"no\": 5083, \"relief\": 8, \"business\": 136, \"men\": 184, \",\": 43415, \"they\": 5200, \"drink\": 264, \"my\": 15171, \"wine\": 106, \"plowman\": 1, \"dig\": 88, \"earth\": 180, \"none\": 102, \"were\": 912, \"level\": 45, \"on\": 9773, \"mind\": 871, \"nobody\": 588, \"up\": 6341, \"at\": 2758, \"his\": 1750, \"word\": 257, \"hey\": 1145, \"reason\": 210, \"excited\": 28, \"he\": 3802, \"kindly\": 12, \"spoke\": 47, \"are\": 2515, \"many\": 433, \"among\": 37, \"us\": 1353, \"who\": 1867, \"feel\": 1619, \"that\": 11539, \"life\": 1595, \"is\": 7287, \"but\": 5695, \"a\": 20617, \"joke\": 42, \"and\": 23103, \"this\": 5083, \"not\": 2793, \"our\": 1113, \"fate\": 70, \"so\": 5964, \"let\": 2816, \"stop\": 983, \"talkin\": 276, \"falsely\": 3, \"now\": 3965, \"hour\": 119, \"getting\": 361, \"late\": 307, \"all\": 7435, \"along\": 200, \"watchtower\": 2, \"princes\": 2, \"kept\": 88, \"view\": 44, \"while\": 615, \"women\": 214, \"came\": 593, \"went\": 488, \"barefoot\": 7, \"servants\": 5, \"outside\": 138, \"in\": 13187, \"cold\": 388, \"distance\": 72, \"wildcat\": 3, \"did\": 903, \"growl\": 5, \"two\": 673, \"riders\": 11, \"approaching\": 5, \"wind\": 200, \"began\": 65, \"howl\": 8, \"intro\": 30, \"joe\": 45, \"where\": 1750, \"you\": 34759, \"going\": 743, \"with\": 6257, \"gun\": 231, \"your\": 9028, \"hand\": 625, \"?\": 4686, \"verse\": 261, \"m\": 7479, \"down\": 3457, \"shoot\": 154, \"old\": 743, \"lady\": 257, \"know\": 5643, \"caught\": 188, \"her\": 2870, \"messing\": 34, \"around\": 1258, \"another\": 766, \"man\": 2166, \"ain\": 1805, \"cool\": 246, \"heard\": 432, \"shot\": 330, \"woman\": 324, \"ground\": 218, \"yes\": 555, \"town\": 354, \"gave\": 299, \"!\": 3317, \"guitar\": 51, \"solo\": 30, \"bridge\": 131, \"alright\": 484, \"one\": 3349, \"more\": 1772, \"time\": 2526, \"baby\": 3837, \"run\": 529, \"go\": 3792, \"well\": 1059, \"it\": 16525, \"south\": 102, \"mexico\": 21, \"free\": 520, \"find\": 737, \"me\": 15874, \"hangman\": 1, \"gonna\": 1700, \"put\": 1136, \"rope\": 36, \"better\": 1105, \"believe\": 662, \"right\": 2077, \"got\": 5348, \"outro\": 31, \"goodbye\": 129, \"everybody\": 906, \"ow\": 67, \"uh\": 618, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-composer",
   "metadata": {},
   "source": [
    "### get_config 란 메소드를 통해서 토크나이저의 여러 정보를 불러왔다. <br/> 그 안에 word_counts 라는 딕셔너리가 보였다.\n",
    "\n",
    "### 그래서 word_counts 딕셔너리만 불러와 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "framed-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_word = config['word_counts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-savings",
   "metadata": {},
   "source": [
    "### 단어장에 쓰인 단어와 그 순서가 딕셔너리 구조로 저장되어있는 것을 확인했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "earlier-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "### print(config_word) 너무 길어서 짤라서 올리겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-fraud",
   "metadata": {},
   "source": [
    "{\"<'start'>\": 156174, \"there\": 3040, \"must\": 509, \"be\": 6646, \"some\": 1847, \"kind\": 224, \"of\": 8964, \"way\": 2081, \"outta\": 145, \"here\": 1914, \"<'end'>\": 156174, \"said\": 1485, \"the\": 39314, \"joker\": 7, \"to\": 20368, \"thief\": 31, \"s\": 11126, \"too\": 1681, \"much\": 818, \"confusion\": 25, \"i\": 45489, \"can\": 5616, \"t\": 11207, \"get\": 4928, \"no\": 5083, \"relief\": 8, \"business\": 136, \"men\": 184, \",\": 43415, \"they\": 5200, \"drink\": 264, \"my\": 15171, \"wine\": 106, \"plowman\": 1, \"dig\": 88, \"earth\": 180, \"none\": 102, \"were\": 912, \"level\": 45, \"on\": 9773, \"mind\": 871, \"nobody\": 588, \"up\": 6341, \"at\": 2758, \"his\": 1750, \"word\": 257, \"hey\": 1145, \"reason\": 210, \"excited\": 28, \"he\": 3802, \"kindly\": 12, \"spoke\": 47, \"are\": 2515, \"many\": 433, \"among\": 37, \"us\": 1353, \"who\": 1867, \"feel\": 1619, \"that\": 11539, \"life\": 1595, \"is\": 7287, \"but\": 5695, \"a\": 20617, \"joke\": 42, \"and\": 23103, \"this\": 5083, \"not\": 2793, \"our\": 1113, \"fate\": 70, \"so\": 5964, \"let\": 2816, \"stop\": 983, \"talkin\": 276, \"falsely\": 3, \"now\": 3965, \"hour\": 119, \"getting\": 361, \"late\": 307, \"all\": 7435, \"along\": 200, \"watchtower\": 2, \"princes\": 2, \"kept\": 88, \"view\": 44, \"while\": 615, \"women\": 214, \"came\": 593, \"went\": 488, \"barefoot\": 7, \"servants\": 5, \"outside\": 138, \"in\": 13187, \"cold\": 388, \"distance\": 72, \"wildcat\": 3, \"did\": 903, \"growl\": 5, \"two\": 673, \"riders\": 11, \"approaching\": 5, \"wind\": 200, \"began\": 65, \"howl\": 8, \"intro\": 30, \"joe\": 45, \"where\": 1750, \"you\": 34759, \"going\": 743, \"with\": 6257, \"gun\": 231, \"your\": 9028, \"hand\": 625, \"?\": 4686, \"verse\": 261, \"m\": 7479, \"down\": 3457, \"shoot\": 154, \"old\": 743, \"lady\": 257, \"know\": 5643, \"caught\": 188, \"her\": 2870, \"messing\": 34, \"around\": 1258, \"another\": 766, \"man\": 2166, \"ain\": 1805, \"cool\": 246, \"heard\": 432, \"shot\": 330, \"woman\": 324, \"ground\": 218, \"yes\": 555, \"town\": 354, \"gave\": 299, \"!\": 3317, \"guitar\": 51, \"solo\": 30, \"bridge\": 131, \"alright\": 484, \"one\": 3349, \"more\": 1772, \"time\": 2526, \"baby\": 3837, \"run\": 529, \"go\": 3792, \"well\": 1059, \"it\": 16525, \"south\": 102, \"mexico\": 21, \"free\": 520, \"find\": 737, \"me\": 15874, \"hangman\": 1, \"gonna\": 1700, \"put\": 1136, \"rope\": 36, \"better\": 1105, \"believe\": 662, \"right\": 2077, \"got\": 5348, \"outro\": 31, \"goodbye\": 129, \"everybody\": 906, \"ow\": 67, \"uh\": 618, \"purple\": 95, \"haze\": 18, \"brain\": 164, \"lately\": 47, \"things\": 911, \"don\": 5140, \"seem\": 178, \"same\": 515, \"actin\": 57, \"funny\": 122, \"why\": 1466, \"excuse\": 63, \"kiss\": 353, \"sky\": 532, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-minute",
   "metadata": {},
   "source": [
    "### 밸류값 순서로 정렬해 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "homeless-enzyme",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4e3ab92a5f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "sorted(config_word.items(), key=lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-stopping",
   "metadata": {},
   "source": [
    "### 에러가 발생했고, dict 타입이 아니라 str 타입이라는것 같다. \n",
    "\n",
    "### 혹시 몰라 타입을 조회해 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prompt-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(config_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-grace",
   "metadata": {},
   "source": [
    "### dict 구조의 str이라는것을 확인했다.\n",
    "\n",
    "### 그래서 이러한 str을 dict으로 바꿔주는 함수가 있어 적용해 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "legislative-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_word_d = eval(config_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "central-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(config_word_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-mistress",
   "metadata": {},
   "source": [
    "### dict으로 잘 변경되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "attached-midnight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<start>', 156174), ('<end>', 156174), ('i', 45489), (',', 43415), ('the', 39314), ('you', 34759), ('and', 23103), ('a', 20617), ('to', 20368), ('it', 16525)]\n"
     ]
    }
   ],
   "source": [
    "sorted_word = sorted(config_word_d.items(), key=lambda x : x[1], reverse=True)\n",
    "print(sorted_word[:10]) ### 너무 길어 10개만 올리겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-tours",
   "metadata": {},
   "source": [
    "### 정렬하고 출력해봤더니 단어장의 중요도와 순서는 단어의 개수로 판단하는것을 알수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-above",
   "metadata": {},
   "source": [
    "# 5. 소스 문장 생성\n",
    "\n",
    "### 문장의 제일 뒤에 있는 단어와 앞에 있는 단어를 제거해 소스 문장을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eleven-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  62 271  27  94 546  20  86 743  90   3   0   0   0]\n",
      "[ 62 271  27  94 546  20  86 743  90   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-facing",
   "metadata": {},
   "source": [
    "# 6. 데이터 분류\n",
    "\n",
    "### 학습과 테스트를 위해 4:1 비율로 트레이닝셋과 테스트셋을 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collected-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "buried-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124939, 14)\n",
      "Target Train: (124939, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-imaging",
   "metadata": {},
   "source": [
    "### 예제에 있는 단어보다 적게 나왔다.\n",
    "\n",
    "### 이유는 정제된 문장이 단어가 0개인 경우가 있었는데 이 경우를 순서를 바꿔 정제 후 단어가 0개인 문장을 제거하니 더 적게 나왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-purpose",
   "metadata": {},
   "source": [
    "# 7. 버퍼사이즈와 배치사이즈를 정합니다.\n",
    "\n",
    "### 검색을 하다 보니 배치사이즈와 에폭이 같이 나오는 경우가 많아 같이 검색해서 아래의 블로그를 참고했습니다.<br/>https://m.blog.naver.com/qbxlvnf11/221449297033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arbitrary-synthesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-pattern",
   "metadata": {},
   "source": [
    "# 8. 모델 설계\n",
    "\n",
    "### 저번 익스와 다르게 함수형인것을 TF 마스터 수업에서 조원의 도움으로 알게 되어 아래 블로그를 참고했습니다.<br/>https://www.tensorflow.org/guide/keras/functional?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "comprehensive-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bridal-artwork",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.24512342e-04,  5.83188485e-05, -1.61438948e-05, ...,\n",
       "          1.38139090e-04,  3.00310512e-05, -1.84129050e-04],\n",
       "        [ 5.46288735e-04,  1.03350227e-04,  3.64425272e-04, ...,\n",
       "          2.15088381e-04, -4.69709230e-05, -2.62921996e-04],\n",
       "        [ 1.19165110e-03,  5.69949625e-04,  3.74548894e-04, ...,\n",
       "          2.98361963e-04,  2.57919019e-04, -8.58039930e-05],\n",
       "        ...,\n",
       "        [ 1.77767430e-03,  3.20719410e-04, -1.61111145e-03, ...,\n",
       "          2.25838786e-03,  1.84631845e-05,  1.15446041e-04],\n",
       "        [ 1.67242880e-03,  7.27213221e-04, -2.20229500e-03, ...,\n",
       "          2.65908451e-03, -4.29614156e-05, -3.32463009e-04],\n",
       "        [ 1.58718706e-03,  1.12879463e-03, -2.82804877e-03, ...,\n",
       "          3.01699201e-03, -6.26632973e-05, -8.78968916e-04]],\n",
       "\n",
       "       [[ 1.24512342e-04,  5.83188485e-05, -1.61438948e-05, ...,\n",
       "          1.38139090e-04,  3.00310512e-05, -1.84129050e-04],\n",
       "        [ 1.27307067e-04,  1.12269969e-04, -2.12032173e-04, ...,\n",
       "         -2.50324138e-05,  2.67426192e-04,  2.25439071e-04],\n",
       "        [ 2.23244897e-05, -2.59466178e-04, -2.78879743e-04, ...,\n",
       "         -4.39389347e-04,  4.36470320e-04,  5.85100614e-04],\n",
       "        ...,\n",
       "        [-1.47812447e-04, -8.38979788e-04, -2.56678951e-03, ...,\n",
       "          1.80105085e-03,  1.23251029e-05, -1.43569626e-03],\n",
       "        [ 1.53130004e-05, -3.14789882e-04, -3.19257309e-03, ...,\n",
       "          2.37711333e-03,  8.87782226e-05, -2.08219863e-03],\n",
       "        [ 1.61115546e-04,  1.89468148e-04, -3.80096678e-03, ...,\n",
       "          2.89113517e-03,  1.84998891e-04, -2.69663101e-03]],\n",
       "\n",
       "       [[ 1.24512342e-04,  5.83188485e-05, -1.61438948e-05, ...,\n",
       "          1.38139090e-04,  3.00310512e-05, -1.84129050e-04],\n",
       "        [ 2.40682653e-04,  2.62513408e-04,  1.77630591e-05, ...,\n",
       "          2.30038815e-04, -1.70192579e-04, -1.84128003e-04],\n",
       "        [ 3.34820681e-04,  1.45569371e-04, -3.85337422e-04, ...,\n",
       "          1.12132053e-04, -1.08020402e-04, -3.34490149e-04],\n",
       "        ...,\n",
       "        [ 4.30328277e-04,  9.87646054e-04, -4.13881661e-03, ...,\n",
       "          2.78187450e-03,  3.10424366e-04, -2.33927695e-03],\n",
       "        [ 5.49647666e-04,  1.37739163e-03, -4.63378290e-03, ...,\n",
       "          3.27431969e-03,  3.82806960e-04, -2.97455094e-03],\n",
       "        [ 6.54138043e-04,  1.71927165e-03, -5.06838597e-03, ...,\n",
       "          3.70103610e-03,  4.64712444e-04, -3.56596196e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.24512342e-04,  5.83188485e-05, -1.61438948e-05, ...,\n",
       "          1.38139090e-04,  3.00310512e-05, -1.84129050e-04],\n",
       "        [ 9.09179726e-05,  1.89726372e-04, -4.51797423e-05, ...,\n",
       "         -1.10571869e-04,  6.10334566e-04, -3.46027547e-04],\n",
       "        [-3.57422221e-04,  1.49318614e-04,  1.57041577e-04, ...,\n",
       "         -4.76472196e-04,  4.35284048e-04, -5.56757324e-04],\n",
       "        ...,\n",
       "        [-8.34220846e-04,  2.28877435e-03, -3.48571292e-03, ...,\n",
       "          8.99788924e-04, -5.69029507e-05, -7.79382535e-04],\n",
       "        [-5.45907184e-04,  2.56960792e-03, -4.09300579e-03, ...,\n",
       "          1.58893794e-03, -7.76467186e-06, -1.30473496e-03],\n",
       "        [-2.77008512e-04,  2.81201722e-03, -4.65655467e-03, ...,\n",
       "          2.20691925e-03,  5.46555784e-05, -1.87979848e-03]],\n",
       "\n",
       "       [[ 1.24512342e-04,  5.83188485e-05, -1.61438948e-05, ...,\n",
       "          1.38139090e-04,  3.00310512e-05, -1.84129050e-04],\n",
       "        [ 8.40334469e-05, -5.26235905e-04, -1.07530264e-04, ...,\n",
       "          2.18385452e-04, -2.15130804e-05, -8.27631215e-04],\n",
       "        [ 9.08691654e-05, -1.05398579e-03, -1.28517378e-04, ...,\n",
       "          3.76004318e-04, -2.51800724e-04, -1.42299093e-03],\n",
       "        ...,\n",
       "        [ 9.50116519e-05, -9.24980675e-04, -1.45647419e-03, ...,\n",
       "         -2.26480974e-04, -7.21845543e-04, -8.41183239e-04],\n",
       "        [ 4.62151547e-05, -3.98999924e-04, -1.73581659e-03, ...,\n",
       "          4.28812316e-04, -5.66998322e-04, -8.35106010e-04],\n",
       "        [ 9.72940834e-05,  8.64014146e-05, -2.12167110e-03, ...,\n",
       "          1.06856693e-03, -4.69104736e-04, -1.05876604e-03]],\n",
       "\n",
       "       [[ 1.24512342e-04,  5.83188485e-05, -1.61438948e-05, ...,\n",
       "          1.38139090e-04,  3.00310512e-05, -1.84129050e-04],\n",
       "        [ 3.67470697e-04,  4.56637237e-04,  1.63636316e-04, ...,\n",
       "          3.30251612e-04,  1.45820013e-04, -5.58313157e-04],\n",
       "        [ 3.63660656e-04,  2.56163738e-04,  2.70314165e-04, ...,\n",
       "         -4.36322216e-06,  2.65134353e-04, -5.68037154e-04],\n",
       "        ...,\n",
       "        [ 9.59426980e-04,  2.12221872e-03,  1.79758319e-03, ...,\n",
       "         -1.64409133e-03,  2.06395076e-03, -1.06034894e-03],\n",
       "        [ 4.65955411e-04,  2.29793345e-03,  1.15236838e-03, ...,\n",
       "         -1.29856018e-03,  1.97159429e-03, -5.91163815e-04],\n",
       "        [ 2.31336890e-04,  2.37012818e-03,  4.22331679e-04, ...,\n",
       "         -7.19689822e-04,  1.68124214e-03, -3.94008617e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "southwest-remove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  20979712  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 85,276,897\n",
      "Trainable params: 85,276,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continued-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3905/3905 [==============================] - 930s 238ms/step - loss: 3.2139 - val_loss: 2.6266\n",
      "Epoch 2/4\n",
      "3905/3905 [==============================] - 929s 238ms/step - loss: 2.4055 - val_loss: 2.3763\n",
      "Epoch 3/4\n",
      "3905/3905 [==============================] - 929s 238ms/step - loss: 1.9107 - val_loss: 2.2413\n",
      "Epoch 4/4\n",
      "3905/3905 [==============================] - 928s 238ms/step - loss: 1.5424 - val_loss: 2.1982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f97d0457b90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=4, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-mileage",
   "metadata": {},
   "source": [
    "# epoch 5\n",
    "\n",
    "embedding_size = 128\n",
    "<br/>hidden_size = 1024\n",
    "<br/>loss: 1.9839 - val_loss: 2.4296\n",
    "\n",
    "embedding_size = 256\n",
    "<br/>hidden_size = 1024\n",
    "<br/>loss: 1.9083 - val_loss: 2.3947\n",
    "\n",
    "embedding_size = 512\n",
    "<br/>hidden_size = 1024\n",
    "<br/>loss: 1.8408 - val_loss: 2.3680\n",
    "\n",
    "embedding_size = 1024\n",
    "<br/>hidden_size = 1024\n",
    "<br/>loss: 1.8163 - val_loss: 2.3648\n",
    "\n",
    "embedding_size = 256\n",
    "<br/>hidden_size = 2048\n",
    "<br/>loss: 1.3801 - val_loss: 2.2346\n",
    "\n",
    "embedding_size = 512\n",
    "<br/>hidden_size = 2048\n",
    "<br/>loss: 1.3033 - val_loss: 2.2164\n",
    "\n",
    "embedding_size = 1024\n",
    "<br/>hidden_size = 2048\n",
    "<br/>loss: loss: 1.2835 - val_loss: 2.2091\n",
    "\n",
    "# epoch 4\n",
    "\n",
    "embedding_size = 256\n",
    "<br/>hidden_size = 2048\n",
    "<br/>loss: 1.5385 - val_loss: 2.1943"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-showcase",
   "metadata": {},
   "source": [
    "### val_loss값을 2.2 이하로 낮추기 위해 처음엔 embedding_size와 hidden_size만 바꿔가며 학습하다 hidden_size가 커지면 오버피팅이 일어나서 epoch을 낮춰 2.2이하의 val_loss값을 구했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "detailed-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-concern",
   "metadata": {},
   "source": [
    "# 9. 문장을 만들어 보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "multiple-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love ma little nasty girl <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-protest",
   "metadata": {},
   "source": [
    "### 완전한 문장은 아니지만 어느정도 의미는 알것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-patrick",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "### 임베딩 사이즈와 히든 사이즈를 바꿔가며 학습속도가 엄청나게 달라지는것을 구글링과 직접 몸으로 느낄수 있었습니다.\n",
    "\n",
    "### 하이퍼 파라미터의 중요도를 알수 있었다고 생각하고, 학습속도가 느리더라도 테스트속도와 loss값만 낮다면 좋다고 생각하지만 테스트 속도만 낮추는 방법은 좀더 고민해 봐야 할것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
